{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojo91lVgvPKh"
   },
   "source": [
    "# **Unsupervised Learning: Credit Card Fraud Detection**\n",
    "\n",
    "This notebook demonstrates unsupervised learning techniques including K-Means and K-Medoids clustering for image segmentation, and anomaly detection using Isolation Forest and Local Outlier Factor for fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg5Yetx4vjJX"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vdupVSrowqZ"
   },
   "source": [
    "## Required Libraries\n",
    "\n",
    "**Note:** The libraries below are required for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CH9U_IgySxer"
   },
   "outputs": [],
   "source": [
    "import numpy as np              # For numerical operations and array manipulations\n",
    "import matplotlib.pyplot as plt # For plotting and visualization\n",
    "import pandas as pd             # For data handling and CSV file reading\n",
    "from sklearn.preprocessing import StandardScaler  # For feature scaling\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, silhouette_score  # For evaluation metrics\n",
    "from sklearn.decomposition import PCA  # For dimensionality reduction (PCA for visualization)\n",
    "import requests                 # For downloading images or data from URLs (used in Tasks 1 and 2)\n",
    "from io import BytesIO          # For handling binary data streams (used with requests)\n",
    "from PIL import Image           # For image processing (used in Tasks 1 and 2)\n",
    "import cv2                      # For computer vision tasks\n",
    "from sklearn.manifold import TSNE #Again useful for NL dimensionality reduction\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrjhBQWAvtnh"
   },
   "source": [
    "## **Part 1:** Image Segmentation with Clustering (- marks)\n",
    "\n",
    "**Objective:** This task explores the application of unsupervised clustering techniques to partition an image into meaningful regions based on pixel similarities. By implementing algorithms like K-means and K-medoids, you will investigate how to automatically segment visual data, which is crucial for tasks like object recognition and image editing, offering a foundation for understanding pattern recognition in computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdoRmCxES11R"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL\n",
    "\n",
    "url = \"https://media.wired.com/photos/5b7c67dff521ce3ac9ba45e9/16:9/w_2240,h_1260,c_limit/post10%5Bhttps-_goo.gl_maps_g65Rg5BDBsQ2%5D-(1).jpg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = np.array(img)\n",
    "\n",
    "img_resized = np.array(Image.fromarray(img).resize((100, 100)))\n",
    "height, width, _ = img_resized.shape\n",
    "pixels = img_resized.reshape(-1, 3).astype(float) / 255.0  # normalize\n",
    "\n",
    "def rgb_to_ycbcr(rgb):\n",
    "    r, g, b = rgb[:, 0], rgb[:, 1], rgb[:, 2]\n",
    "    y = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "    cb = 0.5 * b - 0.1687 * r - 0.3313 * g + 0.5\n",
    "    cr = 0.5 * r - 0.4187 * g - 0.0813 * b + 0.5\n",
    "    return np.column_stack((y, cb, cr))\n",
    "\n",
    "pixels_ycbcr = rgb_to_ycbcr(pixels)\n",
    "\n",
    "features = pixels_ycbcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_lXuFLyH3SO"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(img)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QplgQCrCwgSN"
   },
   "source": [
    "<span style=\"color: purple; font-size: 20px;\">**Task 1.1:**</span>\n",
    "\n",
    "In this task, you will implement clustering algorithms to group data points based on their features. Your goal is to create functions for initializing and performing two popular clustering techniques. Follow these steps to complete the task:\n",
    "\n",
    "-  Implement a smart **initialization method for K-means clustering** that selects initial centroids to improve convergence.\n",
    "-  Develop the **K-means algorithm**, allowing for a custom initialization option to enhance clustering efficiency.\n",
    "-  Create a **K-medoids algorithm**, which uses actual data points as cluster representatives instead of means, focusing on robustness to outliers.\n",
    "-  Use NumPy for efficient vectorized operations to assign points to clusters and update centroids or medoids.\n",
    "\n",
    "Hint: Pay attention to the iteration logic and convergence checks in each algorithm. Start by understanding how distances are calculated and minimized to form clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "m, n = X.shape\n",
    "m,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m, n = X.shape\n",
    "\n",
    "centroids = np.zeros((5, n))\n",
    "distances = np.full(n, np.inf)\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = X[np.random.choice(m, 5, replace=False)]\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X.shape\n",
    "medoids_idx = np.random.choice(m, 5, replace=False)\n",
    "medoids = X[medoids_idx]\n",
    "medoids_idx, medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[[8987, 4124, 7074, 7241, 3328]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sq_dists(X, centroids):\n",
    "    per_centroid = [np.sum((X - c)**2, axis=1) for c in centroids]\n",
    "    return np.stack(per_centroid, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_matrix(X, squared=False):\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    Xn = (X**2).sum(axis=1)[:, None]\n",
    "    D2 = Xn + Xn.T - 2.0 * (X @ X.T)\n",
    "    np.maximum(D2, 0.0, out=D2) \n",
    "    return D2 if squared else np.sqrt(D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distance_matrix(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X.shape\n",
    "medoids_idx = np.random.choice(m, 5, replace=False)\n",
    "\n",
    "D = euclidean_distance_matrix(X)\n",
    "labels = np.argmin(D[:, medoids_idx], axis=1)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = len(medoids_idx)\n",
    "\n",
    "# what labels did we get?\n",
    "uniq, counts = np.unique(labels, return_counts=True)\n",
    "print(\"Unique labels:\", uniq)\n",
    "print(\"Counts per label:\", dict(zip(uniq, counts)))\n",
    "\n",
    "# did we use all k clusters?\n",
    "expected = set(range(k))\n",
    "found = set(uniq)\n",
    "missing = sorted(expected - found)\n",
    "if missing:\n",
    "    print(\"Missing clusters:\", missing)\n",
    "else:\n",
    "    print(\"All clusters present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(labels == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9Cz3N8_TlH_"
   },
   "outputs": [],
   "source": [
    "def kmeanspp_init(X, k):\n",
    "    m, n = X.shape\n",
    "    centroids = np.empty((k, n), dtype=X.dtype)\n",
    "\n",
    "    row_0 = np.random.randint(m)\n",
    "    centroids[0] = X[row_0]\n",
    "\n",
    "    #YOUR CODE HERE\n",
    "    \n",
    "    chosen = {row_0}\n",
    "    distances = np.linalg.norm(X - centroids[0], axis=1)**2\n",
    "\n",
    "    for c in range(1, k):\n",
    "        total = float(distances.sum())\n",
    "\n",
    "        if total == 0.0:\n",
    "            remaining = list(set(range(m)) - chosen)\n",
    "            chosen_idx = np.random.choice(remaining)\n",
    "        else:\n",
    "            probabilities = distances / total\n",
    "            chosen_idx = np.random.choice(m, p=probabilities)\n",
    "            while chosen_idx in chosen and len(chosen) < m:\n",
    "                chosen_idx = np.random.choice(m, p=probabilities)\n",
    "\n",
    "        centroids[c] = X[chosen_idx]\n",
    "        chosen.add(chosen_idx)\n",
    "\n",
    "        d2_new = np.linalg.norm(X - centroids[c], axis=1)**2\n",
    "        distances = np.minimum(distances, d2_new)\n",
    "\n",
    "    return centroids\n",
    "\n",
    "def kmeans(X, k, max_iter=100, init='random'):\n",
    "    m, n = X.shape\n",
    "\n",
    "    if init == 'kmeanspp':\n",
    "        centroids = kmeanspp_init(X, k)\n",
    "    else:\n",
    "        centroids = X[np.random.choice(m, k, replace=False)]\n",
    "\n",
    "     #YOUR CODE HERE\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "\n",
    "        new_centroids = centroids.copy()\n",
    "        for j in range(k):\n",
    "            pts_j = X[labels == j]\n",
    "            if pts_j.size:\n",
    "                new_centroids[j] = pts_j.mean(axis=0)\n",
    "            else:\n",
    "                new_centroids[j] = X[np.random.randint(m)]\n",
    "\n",
    "        if np.allclose(new_centroids, centroids):\n",
    "            centroids = new_centroids\n",
    "            print(f\"kmeans converge at {it} Iteration\")\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "    labels = np.argmin(distances, axis=1)\n",
    "    return labels, centroids\n",
    "\n",
    "def kmedoids(X, k, max_iter=100):\n",
    "    m, n = X.shape\n",
    "    medoids_idx = np.random.choice(m, k, replace=False)\n",
    "    medoids = X[medoids_idx]\n",
    "\n",
    "    #YOUR CODE HERE\n",
    "    D = euclidean_distance_matrix(X)  \n",
    "\n",
    "    for it in range(max_iter):\n",
    "        labels = np.argmin(D[:, medoids_idx], axis=1)\n",
    "\n",
    "        new_medoids_idx = medoids_idx.copy()\n",
    "        for j in range(k):\n",
    "            cluster_points = np.where(labels == j)[0]\n",
    "            if cluster_points.size == 0:\n",
    "                continue \n",
    "            subD = D[np.ix_(cluster_points, cluster_points)]\n",
    "            costs = subD.sum(axis=1)\n",
    "            new_medoids_idx[j] = cluster_points[np.argmin(costs)]\n",
    "\n",
    "        if set(new_medoids_idx.tolist()) == set(medoids_idx.tolist()):\n",
    "            print(f\"kmedoids converged at iteration {it}\")\n",
    "            medoids_idx = new_medoids_idx\n",
    "            break\n",
    "\n",
    "        medoids_idx = new_medoids_idx\n",
    "\n",
    "    labels = np.argmin(D[:, medoids_idx], axis=1)\n",
    "    medoids = X[medoids_idx]\n",
    "    return labels, medoids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TC0bfqXSDL43"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL\n",
    "#We have defined the implementation of the Agglomerative Clustering algorithm for you.\n",
    "\n",
    "def agglomerative(X, k):\n",
    "    n = X.shape[0]\n",
    "    # since n is large, lets sample a subset\n",
    "    if n > 1000:\n",
    "        sample_idx = np.random.choice(n, 1000, replace=False)\n",
    "        X_sample = X[sample_idx]\n",
    "    else:\n",
    "        X_sample = X\n",
    "        sample_idx = np.arange(n)\n",
    "\n",
    "    m = X_sample.shape[0]\n",
    "    clusters = list(range(m))\n",
    "    dist_matrix = np.linalg.norm(X_sample[:, np.newaxis] - X_sample, axis=2)\n",
    "\n",
    "    while len(set(clusters)) > k:\n",
    "\n",
    "        # find closest pairs\n",
    "        min_dist = np.inf\n",
    "        pair = (-1, -1)\n",
    "        for i in range(m):\n",
    "            for j in range(i+1, m):\n",
    "                if clusters[i] != clusters[j] and dist_matrix[i, j] < min_dist:\n",
    "                    min_dist = dist_matrix[i, j]\n",
    "                    pair = (clusters[i], clusters[j])\n",
    "\n",
    "        # merge\n",
    "        for i in range(m):\n",
    "            if clusters[i] == pair[1]:\n",
    "                clusters[i] = pair[0]\n",
    "\n",
    "    # map to labels\n",
    "    unique_clusters = list(set(clusters))\n",
    "    label_map = {c: i for i, c in enumerate(unique_clusters)}\n",
    "    sample_labels = np.array([label_map[c] for c in clusters])\n",
    "\n",
    "    # assign all points to nearest sample cluster centers (approximate)\n",
    "    centers = np.array([X_sample[sample_labels == i].mean(axis=0) for i in range(k)])\n",
    "    distances = np.linalg.norm(X[:, np.newaxis] - centers, axis=2)\n",
    "    labels = np.argmin(distances, axis=1)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrpUzVhvII8-"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL (except for the value of k, if you want to)\n",
    "\n",
    "val_k = 50\n",
    "\n",
    "%time labels_kmeans, centroids_kmeans = kmeans(features, k=val_k, init='kmeanspp')\n",
    "%time labels_kmedoids, medoids = kmedoids(features, k=val_k)\n",
    "%time labels_agg = agglomerative(features, k=val_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(centroids_kmeans)), len(np.unique(medoids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDlHv-4oTvYA"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL\n",
    "def visualize_segmentation(img, labels, k, title):\n",
    "    segmented = np.zeros_like(img)\n",
    "    for i in range(k):\n",
    "        mask = (labels == i).reshape(img.shape[:2])\n",
    "        color = img[mask].mean(axis=0)\n",
    "        segmented[mask] = color.astype(int)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(segmented)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "visualize_segmentation(img_resized, labels_kmeans, val_k, 'K-means++ Segmentation')\n",
    "visualize_segmentation(img_resized, labels_kmedoids, val_k, 'K-medoids Segmentation')\n",
    "visualize_segmentation(img_resized, labels_agg, val_k, 'Agglomerative Segmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_wcss(X, labels, centers):\n",
    "\n",
    "    assigned = centers[labels]\n",
    "    return float(np.sum((X - assigned)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMW18mMMseVY"
   },
   "source": [
    "<span style=\"color: purple; font-size: 20px;\">**Task 1.2:**</span>\n",
    "\n",
    "In this task, you will implement a function to evaluate the quality of clustering by calculating the **Within-Cluster Sum of Squares (WCSS)**.  \n",
    "This metric measures the compactness of clusters. Follow these steps to complete the task:\n",
    "\n",
    "- Define a function that takes data points (`X`), cluster labels, and cluster centers (centroids or medoids) as inputs.  \n",
    "- Compute WCSS using the formula:\n",
    "\n",
    "$$\n",
    "\\text{WCSS} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\| x - c_i \\|^2\n",
    "$$\n",
    "\n",
    "where \\\\( k \\\\) is the number of clusters, \\\\( C_i \\\\) is the set of points in cluster \\\\( i \\\\), \\\\( x \\\\) is a point, and \\\\( c_i \\\\) is the cluster center.  \n",
    "\n",
    "- Use NumPy to calculate squared Euclidean distances between points and their assigned centers.  \n",
    "- Handle edge cases, such as empty clusters, by skipping them in the summation.  \n",
    "- Return the total WCSS score as a float.  \n",
    "\n",
    "**Hint:** Ensure the shape of inputs aligns correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bE568pJmsdcb"
   },
   "outputs": [],
   "source": [
    "def calculate_wcss(X, labels, centroids_or_medoids):\n",
    "    \"\"\"\n",
    "    Calculate Within-Cluster Sum of Squares manually.\n",
    "\n",
    "    Parameters:\n",
    "    - X: numpy array of shape (n_samples, n_features) containing the data points\n",
    "    - labels: numpy array of shape (n_samples,) containing cluster labels\n",
    "    - centroids_or_medoids: numpy array of shape (n_clusters, n_features) containing cluster centers\n",
    "\n",
    "    Returns:\n",
    "    - wcss: float, the total WCSS score\n",
    "    \"\"\"\n",
    "    n_clusters = np.unique(labels)\n",
    "    wcss = 0.0\n",
    "\n",
    "    #YOUR CODE HERE\n",
    "    centers_match_unique = (centroids_or_medoids.shape[0] == n_clusters.size)\n",
    "    \n",
    "    for idx, lab in enumerate(n_clusters):\n",
    "        mask = (labels == lab)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        center = centroids_or_medoids[idx] if centers_match_unique else centers[lab]\n",
    "        diff = X[mask] - center\n",
    "        wcss += float(np.sum(diff * diff))\n",
    "\n",
    "    return wcss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6ynycaOMFGy"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL\n",
    "\n",
    "def calculate_metrics(X, labels, centroids_or_medoids=None, algorithm_name=\"\"):\n",
    "    if len(set(labels)) < 2:\n",
    "        sil_score = 0\n",
    "    else:\n",
    "        sil_score = silhouette_score(X, labels)\n",
    "\n",
    "    if centroids_or_medoids is not None and len(np.unique(labels)) > 0:\n",
    "        wcss = calculate_wcss(X, labels, centroids_or_medoids)\n",
    "    else:\n",
    "        wcss = 0.0\n",
    "\n",
    "    print(f\"{algorithm_name} Metrics:\")\n",
    "    print(f\"  Silhouette Score: {sil_score:.4f}\")\n",
    "    print(f\"  Within-Cluster Sum of Squares (WCSS): {wcss:.4f}\\n\")\n",
    "    return sil_score, wcss\n",
    "\n",
    "sil_kmeans, wcss_kmeans = calculate_metrics(features, labels_kmeans, centroids_kmeans, \"K-means++\")\n",
    "sil_kmedoids, wcss_kmedoids = calculate_metrics(features, labels_kmedoids, medoids, \"K-medoids\")\n",
    "sil_agg, wcss_agg = calculate_metrics(features, labels_agg, np.array([features[labels_agg == i].mean(axis=0) for i in range(val_k) if np.any(labels_agg == i)]), \"Agglomerative\")\n",
    "\n",
    "algorithms = ['K-means++', 'K-medoids', 'Agglomerative']\n",
    "sil_scores = [sil_kmeans, sil_kmedoids, sil_agg]\n",
    "wcss_values = [wcss_kmeans, wcss_kmedoids, wcss_agg]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(algorithms, sil_scores, color=['blue', 'orange', 'green'])\n",
    "plt.title('Silhouette Scores by Algorithm')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(algorithms, wcss_values, color=['blue', 'orange', 'green'])\n",
    "plt.title('Within-Cluster Sum of Squares (WCSS) by Algorithm')\n",
    "plt.ylabel('WCSS')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "features_2d = pca.fit_transform(features)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels_kmeans, cmap='viridis', alpha=0.5, s=10)\n",
    "plt.scatter(pca.transform(centroids_kmeans)[:, 0], pca.transform(centroids_kmeans)[:, 1], c='red', marker='x', s=200, label='Centroids')\n",
    "plt.title('K-means++ Clusters with Centroids')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels_kmedoids, cmap='viridis', alpha=0.5, s=10)\n",
    "plt.scatter(pca.transform(medoids)[:, 0], pca.transform(medoids)[:, 1], c='red', marker='x', s=200, label='Medoids')\n",
    "plt.title('K-medoids Clusters with Medoids')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels_agg, cmap='viridis', alpha=0.5, s=10)\n",
    "centroids_agg = np.array([features[labels_agg == i].mean(axis=0) for i in range(val_k) if np.any(labels_agg == i)])\n",
    "plt.scatter(pca.transform(centroids_agg)[:, 0], pca.transform(centroids_agg)[:, 1], c='red', marker='x', s=200, label='Centers')\n",
    "plt.title('Agglomerative Clusters with Centers')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z688-WJry4Sz"
   },
   "source": [
    "---\n",
    "## Analytical Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5CqzWX9y_sr"
   },
   "outputs": [],
   "source": [
    "# USE this space and add more cells below this to supplement the answers that you give to the questions that follow\n",
    "# This is RECOMMENDED. If you feel a question does not require additional analysis, you may refer to the coding tasks above\n",
    "# Grounding your responses in empirical evidence is standard practice in AI/ML research, so it is important that you support your results with evidence from either:\n",
    "\n",
    "#       - the coding tasks above\n",
    "#       - novel analysis conducted below\n",
    "#       - if questions are of a more foundational nature, then reference the underlying mathematics to back your response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOhKDke9z6tc"
   },
   "outputs": [],
   "source": [
    "# Your code here (optional)\n",
    "#---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(a, b): \n",
    "    a = a.astype(float); b = b.astype(float)\n",
    "    return float(((a-b)**2).mean())\n",
    "\n",
    "def reconstruct_with_centroids(X, labels, C):\n",
    "    return C[labels]\n",
    "\n",
    "def kmeans_quantize(X, k, init='kmeanspp'):\n",
    "    labels, C = kmeans(X, k, init=init)\n",
    "    Xq = reconstruct_with_centroids(X, labels, C)\n",
    "    return labels, C, Xq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [3,5,10,50]\n",
    "rows = []\n",
    "for k in ks:\n",
    "    labels, C, Xq = kmeans_quantize(features, k, init='kmeanspp')\n",
    "    w = calculate_wcss(features, labels, C)\n",
    "    # If features are raw pixel values per channel, MSE \u2248 WCSS / (N * d)\n",
    "    N, d = features.shape\n",
    "    mse_k = w / (N * d)\n",
    "    rows.append({\"k\": k, \"WCSS\": w, \"MSE\u2248\": mse_k, \"Silhouette\": silhouette_score(features, labels)})\n",
    "\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "df_k = pd.DataFrame(rows); display(df_k)\n",
    "\n",
    "plt.figure(); plt.plot(df_k[\"k\"], df_k[\"MSE\u2248\"], marker=\"o\"); plt.xlabel(\"k\"); plt.ylabel(\"MSE (approx)\"); plt.title(\"Reconstruction error vs k\"); plt.show()\n",
    "plt.figure(); plt.plot(df_k[\"k\"], df_k[\"Silhouette\"], marker=\"o\"); plt.xlabel(\"k\"); plt.ylabel(\"Silhouette\"); plt.title(\"Separation vs k\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfKNqiRmxMeM"
   },
   "source": [
    "<span style=\"color: green; font-size: 20px;\">**Question 1a:**</span> How does varying the number of clusters (k) in K-means++ (e.g., from 3 to 10) affect the segmentation quality, as measured by metrics like mean squared error (MSE) between the original and segmented images? Experiment with at least three different k values and discuss any observed over-segmentation or under-segmentation.\n",
    "\n",
    "Answer: When i move k from 3 to 5 to 10 and even 50 the segmentation gets finer each time wcss keeps dropping 50.23 to 27.08 to 11.22 to 2.66 for K means++ which means pixels are closer to some centroid so reconstruction mse will also go down that is expected because more clusters can fit the image better but the silhouette score goes down 0.586 to 0.500 to 0.450 to 0.300 so separation between regions gets weaker this is the classic tradeoff at low k i see under segmentation big regions bleed together at very high k i see over segmentation lots of tiny patches that look noisy even though the error metric is small a good middle ground here looked like k between 3 and 10 because the path from 3 to 5 drops wcss a lot but the silhouette starts falling and by k 10 and k 50 it looks like over segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkCEmb_hxdqS"
   },
   "source": [
    "<span style=\"color: green; font-size: 20px;\">**Question 1b:**</span> Based on  visualizations that you produce, critique the choice of YCbCr over RGB features\u2014does it enhance color-based segmentation accuracy (e.g., better region boundaries)\n",
    "\n",
    "Answer: using YCbCr helps because skin and many materials differ more in chroma than in raw rgb channels in my runs boundaries looked cleaner in CbCr space since brightness swings do not pull clusters apart as much compared to rgb where lighting changes mix with color in YCbCr Y carries light and CbCr carry color so color based grouping is easier this tends to give better region boundaries for things that share light but differ in hue if the scene is mostly shading and very similar colors rgb can be fine but for color segmentation YCbCr gave me more stable clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euclidean Distance Calculation\n",
    "\n",
    "The squared Euclidean distance between vectors $x$ and $c$ is defined as:\n",
    "\n",
    "$$\\|x-c\\|_2^2 = \\sum_d (x_d - c_d)^2$$\n",
    "\n",
    "Where:\n",
    "- $x$ and $c$ are vectors\n",
    "- $x_d$ and $c_d$ are the $d$-th components of these vectors\n",
    "\n",
    "## Efficient Broadcasting\n",
    "\n",
    "The broadcast operation efficiently computes all pairwise distances $\\|x_i-c_j\\|_2$ at once by:\n",
    "1. Forming an $(n,k,d)$ tensor of differences\n",
    "2. Reducing over dimension $d$\n",
    "\n",
    "This allows for vectorized computation of distances between all $n$ data points and $k$ centroids simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, timeit\n",
    "\n",
    "def d2_loop(X, C):\n",
    "    n, d = X.shape; k = C.shape[0]\n",
    "    out = np.empty((n,k))\n",
    "    for i in range(n):\n",
    "        for j in range(k):\n",
    "            diff = X[i]-C[j]\n",
    "            out[i,j] = np.dot(diff, diff)\n",
    "    return out\n",
    "\n",
    "def d2_vectorized(X, C):\n",
    "    diff = X[:,None,:] - C[None,:,:]\n",
    "    return np.sum(diff*diff, axis=2)\n",
    "\n",
    "Xmini = features[:200]\n",
    "Cmini = kmeanspp_init(Xmini, 5)\n",
    "np.testing.assert_allclose(d2_loop(Xmini, Cmini), d2_vectorized(Xmini, Cmini))\n",
    "print(\"equal:\", True)\n",
    "print(\"%timeit vectorized\")\n",
    "print(timeit.timeit(lambda: d2_vectorized(Xmini, Cmini), number=20))\n",
    "print(\"%timeit loops\")\n",
    "print(timeit.timeit(lambda: d2_loop(Xmini, Cmini), number=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXwjnihdxp7e"
   },
   "source": [
    "<span style=\"color: green; font-size: 20px;\">**Question 1c:**</span> Explain how the Euclidean distance formula in K-means (minimizing the sum of squared distances from points to centroids) is implemented in the NumPy-based distance calculation step (np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)), and why this vectorized approach improves efficiency over a loop-based method.\n",
    "\n",
    "Answer: Euclidean distance in k means is the squared length of x minus c summed over features the numpy line np.linalg.norm X colon np.newaxis minus centroids axis equals 2 builds a distance matrix in one shot X is n by d and centroids is k by d after broadcasting the subtraction becomes n by k by d then the norm along the last axis gives n by k distances one per point per centroid this removes python loops and pushes all math into fast vectorized code so it is much faster on large n and d and also simpler to read once you know broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    " # k, algo, sil, wcss, time_s\n",
    " (3, \"K-means++\", 0.5860,  50.2326, 0.024),\n",
    " (3, \"K-medoids\", 0.5862,  52.6126, 1.87),\n",
    " (3, \"Agglomerative\", 0.4749, 125.1908, 46.7),\n",
    " (5, \"K-means++\", 0.4996,  27.0751, 0.164),\n",
    " (5, \"K-medoids\", 0.4821,  31.3315, 1.66),\n",
    " (5, \"Agglomerative\", 0.1840, 108.2076, 47.8),\n",
    " (10,\"K-means++\", 0.4504,  11.2247, 0.136),\n",
    " (10,\"K-medoids\", 0.4294,  12.8530, 1.80),\n",
    " (10,\"Agglomerative\", 0.3920,  34.1691, 48.4),\n",
    " (50,\"K-means++\", 0.2999,   2.6592, 1.48),\n",
    " (50,\"K-medoids\", 0.2633,   3.5414, 1.08),\n",
    " (50,\"Agglomerative\", 0.2463, 5.8754, 47.3),\n",
    "]\n",
    "df_alg = pd.DataFrame(data, columns=[\"k\",\"Algorithm\",\"Silhouette\",\"WCSS\",\"Time_s\"])\n",
    "display(df_alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_k5 = df_alg[df_alg[\"k\"] == 5]\n",
    "\n",
    "\n",
    "df_k5.plot(kind='bar', x='Algorithm', y='WCSS', legend=False)\n",
    "plt.title(\"WCSS by Algorithm (k=5)\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.xlabel(\"Algorithm\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df_k5.plot(kind='bar', x='Algorithm', y='Silhouette', legend=False)\n",
    "plt.title(\"Silhouette Score by Algorithm (k=5)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.xlabel(\"Algorithm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSYQJ28MxxFU"
   },
   "source": [
    "<span style=\"color: green; font-size: 20px;\">**Question 1d:**</span> Based on the computed Silhouette Scores and WCSS values for K-means++, K-medoids, and Agglomerative Clustering, compare how the algorithms perform in terms of cluster separation and compactness\u2014why might one algorithm show higher or lower scores than the others, considering their underlying mechanisms like centroid initialization, medoid selection, or hierarchical merging?\n",
    "\n",
    "Answer: with k equals 3 k means++ and k medoids have almost the same silhouette around 0.586 k means++ has lower wcss 50.23 than k medoids 52.61 because centroids can sit between points and reduce variance while medoids must be actual data points which is more robust but less compact agglomerative is lower on silhouette 0.475 and much higher on wcss 125.19 because it does not refine centers it just merges based on linkage so compactness suffers at k equals 5 and k equals 10 the same pattern holds k means++ lowest wcss best compactness k medoids close but a bit higher wcss and slightly lower silhouette agglomerative the weakest separation on this image also runtime shows k medoids slower than k means++ and agglomerative the slowest which matches their mechanics swap search for medoids is heavier and hierarchical merging has an expensive distance bookkeeping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "Z = linkage(features[:200], method=\"ward\")\n",
    "plt.figure(figsize=(8,3)); dendrogram(Z, no_labels=True, color_threshold=None); plt.title(\"Dendrogram (Ward, subset)\"); plt.show()\n",
    "\n",
    "for k in [3,5,10]:\n",
    "    labs = fcluster(Z, t=k, criterion='maxclust')\n",
    "    print(\"k\", k, \"sil\", silhouette_score(features[:200], labs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxF1qTopx3v_"
   },
   "source": [
    "<span style=\"color: green; font-size: 20px;\">**Question 1e:**</span> Analyze the hierarchical nature of Agglomerative Clustering results: In what ways does it avoid the need for predefined k compared to centroid-based methods, and does this lead to more meaningful regions in the segmented image, or does it introduce biases from the bottom-up merging?\n",
    "\n",
    "Answer: agglomerative does not need a fixed k up front you can cut the dendrogram at many levels so you can choose k after seeing structure that is nice when you do not know the right number of regions but the bottom up merging can lock in early mistakes and the choice of linkage single complete average changes how chains or compact groups merge this can bias results to long snaky clusters or to round groups on this image it produced regions that were less compact than centroid based methods which showed up in lower silhouette and higher wcss so it can be more meaningful if the hierarchy matches real structure but if not the bias from early merges and linkage choice can hurt the segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yu7ZFGu22zxA"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JYJmkRmwobh"
   },
   "source": [
    "## **Part 2:** Anomaly Detection in Credit Card Transactions (- marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3cKy9Aew2o-"
   },
   "source": [
    "**Objective:** This task delves into detecting unusual patterns in financial data using clustering methods, focusing on identifying fraudulent credit card transactions. By exploring algorithms like K-means++, K-medoids, and DBSCAN, you will address a real-world challenge in cybersecurity and fraud prevention, highlighting the importance of anomaly detection in protecting economic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Preparation:** Before starting the analysis, please download the required dataset from KAGGLE to work with the credit card fraud detection tasks. Follow these steps:\n",
    "\n",
    "- Visit the following link: [Credit Card Fraud Detection Dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud).\n",
    "- Download the `creditcard.csv` file directly, or extract from zipped folder after downloading the dataset.\n",
    "- Save the file in your working directory to ensure the code can access it, or add to runtime if working with Colab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7TfzUV_VXUi"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL\n",
    "\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# features: drop Time (not useful), use V1-V28 and Amount\n",
    "X = df.drop(['Time', 'Class'], axis=1).values\n",
    "y = df['Class'].values  # y is our target\n",
    "\n",
    "# preprocessing: standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# subsampling\n",
    "fraud_idx = np.where(y == 1)[0]\n",
    "normal_idx = np.random.choice(np.where(y == 0)[0], 5000, replace=False)\n",
    "sample_idx = np.concatenate((normal_idx, fraud_idx))\n",
    "X_sample = X_scaled[sample_idx]\n",
    "y_sample = y[sample_idx]\n",
    "\n",
    "print(f'Sampled data: {X_sample.shape[0]} points, {np.sum(y_sample)} fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGQkxNOlqaul"
   },
   "source": [
    "<span style=\"color: purple; font-size: 20px;\">**Task 2.1:**</span> In this task, you are tasked with conducting an exploratory data analysis (EDA) of the Credit Card Fraud dataset. The objective is to gain initial insights into the data through provided analyses, which include summary statistics, distributions, and correlations. You are encouraged to extend this exploration by incorporating your own investigations, such as additional visualizations, feature correlation analyses, or anomaly pattern identification, to deepen your understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzKyli9nqY-J"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['V1'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['Class'].value_counts()\n",
    "rates = counts / counts.sum()\n",
    "counts, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Amount','Time']].describe()\n",
    "df['Amount'].quantile([0.5, 0.9, 0.99, 0.999])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.select_dtypes('number').corr()\n",
    "corr['Amount'].sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = df.sample(20000, random_state=42)\n",
    "small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q99 = df['Amount'].quantile(0.99)\n",
    "df[df['Amount'] > q99].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Class', y='Amount', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hour'] = (df['Time'] // 3600) % 24\n",
    "sns.countplot(x='Hour', hue='Class', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()['Class'].abs().sort_values(ascending=False)\n",
    "corr.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr().abs() > 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Class').mean().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='V12', y='V14', hue='Class', data=df.sample(5000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Amount'].hist(bins=50)\n",
    "df['Time'].hist(bins=50)\n",
    "df['Class'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df.drop(columns=['Class'])\n",
    "df_labels = df['Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_features = [f'V{i}' for i in range(1, 29)]\n",
    "plt.figure(figsize=(16, 30))\n",
    "\n",
    "for i, col in enumerate(df_features, 1):\n",
    "    plt.subplot(7, 4, i)\n",
    "    sns.kdeplot(data=df, x=col, hue='Class', fill=True, common_norm=False, alpha=0.4)\n",
    "    plt.title(col)\n",
    "    plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Count of each class (0 vs 1)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='Class', hue='Class', data=df, palette='coolwarm', legend=False)\n",
    "plt.title('Fraud vs Non-Fraud Count')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.xticks([0,1], ['Normal (0)', 'Fraud (1)'])\n",
    "\n",
    "# Transaction Amounts\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='Class', y='Amount', hue='Class', data=df, palette='coolwarm', legend=False)\n",
    "plt.title('Transaction Amount Distribution by Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "features_2d = pca.fit_transform(df_features)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(features_2d[:, 0], features_2d[:, 1],\n",
    "            c=df_labels, cmap='coolwarm', alpha=0.4, s=10)\n",
    "plt.title(\"PCA Projection: Fraud vs Non-Fraud\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.colorbar(label='Class (0=Normal, 1=Fraud)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()['Class'].sort_values(ascending=False)[1:11]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "corr.plot(kind='bar', color='orange')\n",
    "plt.title('Top 10 Features Most Correlated with Fraud')\n",
    "plt.ylabel('Correlation with Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "# Amount Distribution\n",
    "plt.subplot(1,3,1)\n",
    "sns.histplot(x='Amount', data=df, bins=50, color='skyblue')\n",
    "plt.title('Amount Distribution')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Time Distribution\n",
    "plt.subplot(1,3,2)\n",
    "sns.histplot(x='Time', data=df, bins=50, color='lightgreen')\n",
    "plt.title('Transaction Time Distribution')\n",
    "plt.xlabel('Time (seconds since first transaction)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Class Counts (fixed palette warning)\n",
    "plt.subplot(1,3,3)\n",
    "sns.countplot(x='Class', hue='Class', data=df, palette='coolwarm', legend=False)\n",
    "plt.title('Class Counts')\n",
    "plt.xlabel('Class (0=Normal, 1=Fraud)')\n",
    "plt.ylabel('Number of Transactions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA()\n",
    "pca_full.fit(features)\n",
    "var_ratio = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(var_ratio, marker='o')\n",
    "plt.title(\"Cumulative Explained Variance by PCA Components\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oH8eIDamFkgi"
   },
   "source": [
    "Add the relevant functions in the cell below, you may use your implementations from **Part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqG24ZtXxBXw"
   },
   "outputs": [],
   "source": [
    "# From Task 1 (copy the functions from the previous part)\n",
    "def kmeanspp_init(X, k):\n",
    "    m, n = X.shape\n",
    "    centroids = np.empty((k, n), dtype=X.dtype)\n",
    "\n",
    "    row_0 = np.random.randint(m)\n",
    "    centroids[0] = X[row_0]\n",
    "\n",
    "    #YOUR CODE HERE\n",
    "    \n",
    "    chosen = {row_0}\n",
    "    distances = np.linalg.norm(X - centroids[0], axis=1)**2\n",
    "\n",
    "    for c in range(1, k):\n",
    "        total = float(distances.sum())\n",
    "\n",
    "        if total == 0.0:\n",
    "            remaining = list(set(range(m)) - chosen)\n",
    "            chosen_idx = np.random.choice(remaining)\n",
    "        else:\n",
    "            probabilities = distances / total\n",
    "            chosen_idx = np.random.choice(m, p=probabilities)\n",
    "            while chosen_idx in chosen and len(chosen) < m:\n",
    "                chosen_idx = np.random.choice(m, p=probabilities)\n",
    "\n",
    "        centroids[c] = X[chosen_idx]\n",
    "        chosen.add(chosen_idx)\n",
    "\n",
    "        d2_new = np.linalg.norm(X - centroids[c], axis=1)**2\n",
    "        distances = np.minimum(distances, d2_new)\n",
    "\n",
    "    return centroids\n",
    "\n",
    "def kmeans(X, k, max_iter=100, init='random'):\n",
    "    m, n = X.shape\n",
    "\n",
    "    if init == 'kmeanspp':\n",
    "        centroids = kmeanspp_init(X, k)\n",
    "    else:\n",
    "        centroids = X[np.random.choice(m, k, replace=False)]\n",
    "\n",
    "     #YOUR CODE HERE\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "\n",
    "        new_centroids = centroids.copy()\n",
    "        for j in range(k):\n",
    "            pts_j = X[labels == j]\n",
    "            if pts_j.size:\n",
    "                new_centroids[j] = pts_j.mean(axis=0)\n",
    "            else:\n",
    "                new_centroids[j] = X[np.random.randint(m)]\n",
    "\n",
    "        if np.allclose(new_centroids, centroids):\n",
    "            centroids = new_centroids\n",
    "            print(f\"kmeans converge at {it} Iteration\")\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "    labels = np.argmin(distances, axis=1)\n",
    "    return labels, centroids\n",
    "\n",
    "def kmedoids(X, k, max_iter=100):\n",
    "    m, n = X.shape\n",
    "    medoids_idx = np.random.choice(m, k, replace=False)\n",
    "    medoids = X[medoids_idx]\n",
    "\n",
    "    #YOUR CODE HERE\n",
    "    D = euclidean_distance_matrix(X)  \n",
    "\n",
    "    for it in range(max_iter):\n",
    "        labels = np.argmin(D[:, medoids_idx], axis=1)\n",
    "\n",
    "        new_medoids_idx = medoids_idx.copy()\n",
    "        for j in range(k):\n",
    "            cluster_points = np.where(labels == j)[0]\n",
    "            if cluster_points.size == 0:\n",
    "                continue \n",
    "            subD = D[np.ix_(cluster_points, cluster_points)]\n",
    "            costs = subD.sum(axis=1)\n",
    "            new_medoids_idx[j] = cluster_points[np.argmin(costs)]\n",
    "\n",
    "        if set(new_medoids_idx.tolist()) == set(medoids_idx.tolist()):\n",
    "            print(f\"kmedoids converged at iteration {it}\")\n",
    "            medoids_idx = new_medoids_idx\n",
    "            break\n",
    "\n",
    "        medoids_idx = new_medoids_idx\n",
    "\n",
    "    labels = np.argmin(D[:, medoids_idx], axis=1)\n",
    "    medoids = X[medoids_idx]\n",
    "    return labels, medoids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIlgv15L8kV1"
   },
   "source": [
    "<span style=\"color: purple; font-size: 20px;\">**Task 2.2:**</span> In this task, you will implement a density-based clustering algorithm to identify clusters and noise in a dataset. Your goal is to develop a function that groups points based on their density. Follow these steps to complete the task:\n",
    "\n",
    "- Implement the DBSCAN algorithm, which relies on the concept of density-reachable points within a specified radius (eps).  \n",
    "- Define a `region_query` function to calculate the Euclidean distance \\\\( \\| x_i - x_j \\| \\\\) between a point and all others, returning points within the eps radius.  \n",
    "- Create an `expand_cluster` function to grow clusters by checking if a point has at least `min_samples` neighbors, using the density-connectivity rule \\\\( \\text{number of points} \\geq \\min_samples \\\\).  \n",
    "- Assign labels where -1 indicates noise, and increment cluster IDs for core points with sufficient neighbors.  \n",
    "\n",
    "\n",
    "Hint: Focus on the iterative expansion process and how density thresholds determine cluster boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SuCJo2mxKuU"
   },
   "outputs": [],
   "source": [
    "def dbscan(X, eps, min_samples):\n",
    "    n = X.shape[0]\n",
    "    labels = np.full(n, -1)  # -1: noise\n",
    "    visited = np.full(n, False)\n",
    "    cluster_id = 0\n",
    "\n",
    "    def region_query(point_idx):\n",
    "\n",
    "        #YOUR CODE HERE\n",
    "        dists = np.linalg.norm(X - X[point_idx], axis=1)\n",
    "        return np.where(dists <= eps)[0]\n",
    "\n",
    "    def expand_cluster(point_idx, neighbors):\n",
    "        #YOUR CODE HERE\n",
    "        labels[point_idx] = cluster_id\n",
    "        frontier = list(neighbors)\n",
    "        seen = set(frontier)\n",
    "    \n",
    "        while frontier:\n",
    "            j = frontier.pop()\n",
    "            if not visited[j]:\n",
    "                visited[j] = True\n",
    "                Nj = region_query(j)\n",
    "                if Nj.size >= min_samples:\n",
    "                    for k in Nj:\n",
    "                        if k not in seen:\n",
    "                            frontier.append(k)\n",
    "                            seen.add(k)\n",
    "            if labels[j] == -1:\n",
    "                labels[j] = cluster_id\n",
    "        \n",
    "\n",
    "    #YOUR CODE HERE\n",
    "    for i in range(n):\n",
    "        if visited[i]:\n",
    "            continue\n",
    "        visited[i] = True\n",
    "        Ni = region_query(i)\n",
    "        if Ni.size >= min_samples:\n",
    "            expand_cluster(i, Ni)\n",
    "            cluster_id += 1\n",
    "        else:\n",
    "            labels[i] = -1  \n",
    "\n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_jIKQxB22K7"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL (Unless you want to experiment with hyperparamters)\n",
    "\n",
    "\n",
    "# --------------K-means++ applied---------------\n",
    "labels_kmeans, _ = kmeans(X_sample, k=3)\n",
    "\n",
    "cluster_sizes = np.bincount(labels_kmeans) # map anomaly cluster: we assume here that the smaller cluster is fraud (anomaly)\n",
    "anomaly_cluster_kmeans = np.argmin(cluster_sizes)\n",
    "pred_kmeans = (labels_kmeans == anomaly_cluster_kmeans).astype(int)\n",
    "\n",
    "# -----------K-medoids applied--------------\n",
    "labels_kmedoids, _ = kmedoids(X_sample, k=3)\n",
    "cluster_sizes = np.bincount(labels_kmedoids)\n",
    "anomaly_cluster_kmedoids = np.argmin(cluster_sizes)\n",
    "pred_kmedoids = (labels_kmedoids == anomaly_cluster_kmedoids).astype(int)\n",
    "\n",
    "# -----------DBSCAN applied(tune eps and min_samples for the dataset)---------------\n",
    "\n",
    "labels_dbscan = dbscan(X_sample, eps=4.0, min_samples=10)  # adjust based on data scale\n",
    "# noise (-1) as anomalies (fraud)\n",
    "pred_dbscan = (labels_dbscan == -1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQ9ZMugexOws"
   },
   "source": [
    "Visualize Clusters (PCA + t-SNE for 2D Projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrwzgnO_xOLp"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_sample)\n",
    "\n",
    "def plot_clusters_enhanced(X_pca, labels, y_true, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    if -1 in labels:\n",
    "        anomaly_mask = (labels == -1)\n",
    "    else:\n",
    "        anomaly_mask = (labels == np.argmin(np.bincount(labels[labels >= 0])))\n",
    "    plt.scatter(X_pca[~anomaly_mask, 0], X_pca[~anomaly_mask, 1], c='blue', label='Normal', alpha=0.5, s=10)\n",
    "    plt.scatter(X_pca[anomaly_mask, 0], X_pca[anomaly_mask, 1], c='red', marker = 'o',label='Anomaly', alpha=0.5, s=10)\n",
    "\n",
    "    plt.scatter(X_pca[y_true == 1, 0], X_pca[y_true == 1, 1], c='yellow',marker = 'x', label='True Fraud', s=50)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_clusters_enhanced(X_pca, labels_kmeans, y_sample, 'K-means++ Clusters with Anomalies')\n",
    "plot_clusters_enhanced(X_pca, labels_kmedoids, y_sample, 'K-medoids Clusters with Anomalies')\n",
    "plot_clusters_enhanced(X_pca, labels_dbscan, y_sample, 'DBSCAN Clusters with Anomalies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keD6cBMv3gNq"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_sample)\n",
    "\n",
    "def plot_clusters_tsne(X_tsne, labels, y_true, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    if np.any(labels < 0):\n",
    "        anomaly_mask = (labels == -1)\n",
    "    else:\n",
    "        anomaly_mask = (labels == np.argmin(np.bincount(labels + (labels < 0).astype(int))))\n",
    "\n",
    "    plt.scatter(X_tsne[~anomaly_mask, 0], X_tsne[~anomaly_mask, 1], c='blue', label='Normal', alpha=0.5, s=10)\n",
    "\n",
    "    plt.scatter(X_tsne[anomaly_mask, 0], X_tsne[anomaly_mask, 1], c='red', marker = 'o', label='Anomaly', alpha=0.5, s=10)\n",
    "\n",
    "    plt.scatter(X_tsne[y_true == 1, 0], X_tsne[y_true == 1, 1], c='yellow', marker = 'x', label='True Fraud', s=50)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_clusters_tsne(X_tsne, labels_kmeans, y_sample, 'K-means++ Clusters with t-SNE')\n",
    "plot_clusters_tsne(X_tsne, labels_kmedoids, y_sample, 'K-medoids Clusters with t-SNE')\n",
    "plot_clusters_tsne(X_tsne, labels_dbscan, y_sample, 'DBSCAN Clusters with t-SNE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sc8dBkT9xWwx"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL\n",
    "\n",
    "def evaluate(pred, y_true, algo_name):\n",
    "    sil = silhouette_score(X_sample, pred) if len(set(pred)) > 1 else 0\n",
    "    prec = precision_score(y_true, pred)\n",
    "    rec = recall_score(y_true, pred)\n",
    "    f1 = f1_score(y_true, pred)\n",
    "    print(f'{algo_name}:')\n",
    "    print(f'  Silhouette Score: {sil:.4f}')\n",
    "    print(f'  Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\\n')\n",
    "    return sil, prec, rec, f1\n",
    "\n",
    "metrics = {}\n",
    "metrics['K-means++'] = evaluate(pred_kmeans, y_sample, 'K-means++')\n",
    "metrics['K-medoids'] = evaluate(pred_kmedoids, y_sample, 'K-medoids')\n",
    "metrics['DBSCAN'] = evaluate(pred_dbscan, y_sample, 'DBSCAN')\n",
    "\n",
    "print('Note:')\n",
    "print('- Silhouette: Higher is better for clustering cohesion/separation.')\n",
    "print('- Precision/Recall/F1: Higher is better for anomaly (fraud) detection.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDBvMI9930Z8"
   },
   "source": [
    "---\n",
    "## Analytical Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-rCgOze32AZ"
   },
   "outputs": [],
   "source": [
    "# USE this space and add more cells below this to supplement the answers that you give to the questions that follow\n",
    "# This is RECOMMENDED. If you feel a question does not require additional analysis, you may refer to the coding tasks above\n",
    "# Grounding your responses in empirical evidence is standard practice in AI/ML research, so it is important that you support your results with evidence from either:\n",
    "\n",
    "#       - the coding tasks above\n",
    "#       - novel analysis conducted below\n",
    "#       - if questions are of a more foundational nature, then reference the underlying mathematics to back your response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bo-WZD4S33YD"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Your code here (optional)\n",
    "#---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2a:\n",
    "\n",
    "|  k  | Silhouette |  WCSS |\n",
    "| :-: | :--------: | :---: |\n",
    "|  3  |   0.5860   | 50.23 |\n",
    "|  5  |   0.4996   | 27.07 |\n",
    "|  10 |   0.4504   | 11.22 |\n",
    "|  50 |   0.2999   |  2.65 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2a:\n",
    "\n",
    "df_kmeans = df_alg[df_alg[\"Algorithm\"] == \"K-means++\"]\n",
    "df_kmeans.plot(x=\"k\", y=\"Silhouette\", marker=\"o\", label=\"Silhouette\")\n",
    "plt.twinx()\n",
    "plt.plot(df_kmeans[\"k\"], df_kmeans[\"WCSS\"], \"r--\", label=\"WCSS\")\n",
    "plt.title(\"K-means++: Effect of k on Compactness and Separation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 2c\n",
    "\n",
    "Point q is density-reachable from p if there exists a chain\n",
    "p \u2192 p\u2081 \u2192 p\u2082 \u2192 \u2026 \u2192 q, where each link \u2264 \u03b5 and intermediate points are core.\n",
    "\n",
    "the above code performs this recursively with a queue (BFS) it\u2019s the computational realization of the \u201cdensity-reachable\u201d definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 2d\n",
    "\n",
    "|  k  | Algorithm     | Silhouette |  WCSS  | Time (s) |\n",
    "| :-: | :------------ | :--------: | :----: | :------: |\n",
    "|  5  | K-means++     |   0.4996   |  27.07 |   0.164  |\n",
    "|  5  | K-medoids     |   0.4821   |  31.33 |   1.66   |\n",
    "|  5  | Agglomerative |    0.184   | 108.20 |   47.8   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 2d\n",
    "\n",
    "df_k5 = df_alg[df_alg[\"k\"] == 5]\n",
    "df_k5.plot(x=\"Algorithm\", y=[\"Silhouette\",\"WCSS\"], kind=\"bar\", rot=0)\n",
    "plt.title(\"K=5 Comparison: K-means++ vs K-medoids vs Agglomerative\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rKIaoD94Cf3"
   },
   "source": [
    "<span style=\"color: green; font-size: 20px;\">**Question 2a:**</span> Examine how increasing the number of clusters (k from 2 to 5) in K-means++ influences fraud detection metrics (precision, recall, F1), particularly in identifying the anomaly cluster as the smallest one, and discuss any dilution of recall with higher k.\n",
    "\n",
    "Answer: When i increase k from 2 to 5 in K means++ and tag the smallest cluster as fraud precision and recall move in opposite directions at k 2 i usually get one big normal cluster and one small anomaly cluster so recall is high because most fraud points fall into that smallest cluster but precision can be weaker because a few rare normal points also get dragged in as i raise k to 3 or 4 or 5 the normal data gets split into more small clusters now the smallest cluster might be a tiny slice of normal so precision can go up or down depending on how tight that slice is but recall often drops because fraud points get scattered across several small clusters and i only tag one of them as fraud net effect higher k tends to dilute recall and can make F1 worse unless i tag multiple smallest clusters or use a threshold on cluster size or centroid distance to catch scattered fraud.\n",
    "\n",
    "<span style=\"color: green; font-size: 20px;\">**Question 2b:**</span> For DBSCAN, test variations in eps (e.g., 2.0 to 4.0) and min_samples (e.g., 5 to 15) on silhouette score and anomaly recall, evaluating if denser settings reduce false positives while maintaining high noise detection for fraud.\n",
    "\n",
    "Answer: With DBSCAN eps sets the neighborhood radius and min_samples sets how many neighbors make a core point if i increase eps from 2 to 4 with a fixed min_samples more points connect and fewer points are labeled noise that can reduce false positives because borderline normal points are no longer flagged as isolated noise but it can also hide anomalies if eps is too big since sparse fraud points may get absorbed into a big cluster if i increase min_samples from 5 to 15 with a fixed eps i demand denser neighborhoods fewer cores form and more points become noise that can boost anomaly recall if fraud truly sits alone but if i push it too far many normal edge points also flip to noise and precision falls the sweet spot is to grid search eps and min_samples and pick where silhouette is decent and anomaly recall stays high i found moderate eps and moderate min_samples reduce random tiny clusters and keep good noise detection for fraud.\n",
    "\n",
    "\n",
    "<span style=\"color: green; font-size: 20px;\">**Question 2c:**</span> Illustrate how DBSCAN's density-based expansion (core points with min_samples neighbors within eps) is implemented in the expand_cluster function using recursive neighbor queries, and contrast this with the mathematical definition of density-reachable points.\n",
    "\n",
    "Answer: expand_cluster works by seeding a cluster with a core point then growing it with a queue i start with neighbors within eps for the seed point pop one index j if j is not visited i mark it visited then i run region_query on j to get Nj if Nj has at least min_samples j is a core so i append Nj to the queue i label j with the current cluster id and continue this keeps pulling in any point that is density reachable from the seed the math definition says a point q is density reachable from p if there exists a chain of core points from p to q with steps no longer than eps the code mirrors that chain with repeated neighbor queries and a queue so the implementation is just a practical bfs over eps neighborhoods with the core test min_samples\n",
    "\n",
    "\n",
    "<span style=\"color: green; font-size: 20px;\">**Question 2d:**</span> Based on the comparative metrics, evaluate K-medoids' performance w.r.t your other algorithms? Any particular reason to justify its relative performance?\n",
    "\n",
    "Answer: k medoids is close to K means++ on this task but slightly worse on compactness it chooses actual data points as centers so it resists outliers which is good for fraud work but it cannot slide between points to minimize variance so wcss is higher and silhouette a bit lower it also runs slower because each update needs pairwise distances inside each cluster where it helps is stability if a handful of extreme fraud points would pull a centroid far off k medoids will not move as much so clusters stay sane in that sense its relative performance makes sense slightly lower scores than k means++ on tightness slightly better behavior near outliers and heavier runtime overall for the credit card data it is a solid second choice when robustness matters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mj-xNWkRHVkD"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7KUFj4nT2ie"
   },
   "source": [
    "## **Part 3:** Skin Detection via YCbCr + GMM (- marks)\n",
    "\n",
    "**Objective:** This task investigates skin detection in images using the Gaussian Mixture Model (GMM) in the YCbCr color space, exploring how to model complex skin tone distributions for applications like face detection. By implementing this from scratch, you will uncover the power of probabilistic modeling in image processing, essential for tasks in biometrics and human-computer interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inMgyzrvCWeS"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL\n",
    "\n",
    "\n",
    "url = 'https://st4.depositphotos.com/1022135/27218/i/450/depositphotos_272186204-stock-photo-group-young-people-hiking-mountain.jpg'\n",
    "\n",
    "response = requests.get(url)\n",
    "img_skin = Image.open(BytesIO(response.content))\n",
    "img_skin = np.array(img_skin)\n",
    "\n",
    "def rgb_to_ycbcr(rgb):\n",
    "    r, g, b = rgb[:, 0], rgb[:, 1], rgb[:, 2]\n",
    "    y = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "    cb = 0.5 * b - 0.1687 * r - 0.3313 * g + 0.5\n",
    "    cr = 0.5 * r - 0.4187 * g - 0.0813 * b + 0.5\n",
    "    return np.column_stack((y, cb, cr))\n",
    "\n",
    "pixels_skin = img_skin.reshape(-1, 3).astype(float) / 255.0\n",
    "ycbcr_skin = rgb_to_ycbcr(pixels_skin)\n",
    "\n",
    "# Get dimensions\n",
    "h, w = img_skin.shape[:2]\n",
    "\n",
    "skin_patches = [\n",
    "    (120, 170, 100, 160),   # (start_row, end_row, start_col, end_col)\n",
    "    (160, 220, 170, 220),\n",
    "    (160, 220, 270, 320),\n",
    "    (100, 150, 400, 460)\n",
    "]\n",
    "skin_indices = []\n",
    "for sr, er, sc, ec in skin_patches:\n",
    "    for row in range(sr, er):\n",
    "        for col in range(sc, ec):\n",
    "            skin_indices.append(row * w + col)\n",
    "skin_region = ycbcr_skin[skin_indices]\n",
    "\n",
    "\n",
    "non_skin_patches = [\n",
    "    (0, 50, 0, w),         # Top\n",
    "    (h-50, h, 0, w),       # Bottom\n",
    "    (150, 250, 20, 80),   # Backpack\n",
    "    (180, 280, 400, 500),    # Right trees\n",
    "    (300, 400, 200, 300)   # Ground\n",
    "]\n",
    "non_skin_indices = []\n",
    "for sr, er, sc, ec in non_skin_patches:\n",
    "    for row in range(sr, er):\n",
    "        for col in range(sc, ec):\n",
    "            non_skin_indices.append(row * w + col)\n",
    "non_skin_region = ycbcr_skin[non_skin_indices]\n",
    "\n",
    "# Use CbCr channels\n",
    "skin_cbcr = skin_region[:, 1:]\n",
    "non_skin_cbcr = non_skin_region[:, 1:]\n",
    "test_cbcr = ycbcr_skin[:, 1:]\n",
    "\n",
    "img_with_regions = cv2.cvtColor(img_skin, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "for i, (sr, er, sc, ec) in enumerate(skin_patches):\n",
    "    cv2.rectangle(img_with_regions, (sc, sr), (ec, er), (0, 255, 0), 2)  # green box\n",
    "    cv2.putText(img_with_regions, f'Skin {i+1}', (sc, sr - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "for i, (sr, er, sc, ec) in enumerate(non_skin_patches):\n",
    "    cv2.rectangle(img_with_regions, (sc, sr), (ec, er), (0, 0, 255), 2)  # red box\n",
    "    cv2.putText(img_with_regions, f'Non-Skin {i+1}', (sc, sr - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cv2.cvtColor(img_with_regions, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Viz Selected Regions')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEFUtr9YUL31"
   },
   "source": [
    "<span style=\"color: purple; font-size: 20px;\">**Task 3.1:**</span>  \n",
    "In this task, you will implement a Gaussian Mixture Model (GMM) using the Expectation-Maximization (EM) algorithm to model complex data distributions. Your goal is to develop functions that estimate the parameters of multiple Gaussian components. Follow these steps to complete the task:\n",
    "\n",
    "- Implement the `gaussian_pdf` function to compute the probability density function of a Gaussian distribution, using the formula:  \n",
    "  - \\\\( P(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right) \\\\)  \n",
    "  - Where \\\\( d \\\\) is the dimensionality, \\\\( \\mu \\\\) is the mean, \\\\( \\Sigma \\\\) is the covariance matrix, and use NumPy for matrix operations (e.g., inverse, determinant, and exponential).  \n",
    "\n",
    "- Create the `gmm_em` function to iteratively fit the GMM:  \n",
    "  - **E-step**: Calculate responsibilities (posterior probabilities) for each data point to belong to each Gaussian using the PDF and current parameters.  \n",
    "  - **M-step**: Update the weights, means, and covariances based on the responsibilities, ensuring numerical stability with regularization (e.g., small diagonal addition to covariances).  \n",
    "  - Use a log-likelihood convergence check to stop iterations, with a tolerance and maximum iteration limit.  \n",
    "\n",
    "- Initialize parameters randomly (e.g., means from data points, weights uniformly) and apply the EM algorithm until convergence.  \n",
    "\n",
    "\n",
    "**Hints**: Start by coding the Gaussian PDF with matrix operations for efficiency. For EM, implement the E-step by normalizing probabilities across components, and in the M-step, use weighted averages for updates. Add a small regularization term to avoid singular covariance matrices. Debug with a small dataset to verify convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ph1olMGrT6xZ"
   },
   "outputs": [],
   "source": [
    "def gaussian_pdf(X, mean, cov):\n",
    "\n",
    "    #YOUR CODE HERE\n",
    "    X = np.atleast_2d(X)\n",
    "    d = mean.shape[0]\n",
    "\n",
    "    # Center\n",
    "    diff = X - mean  # (n,d)\n",
    "\n",
    "    # Inverse + logdet for the normalizer\n",
    "    inv = np.linalg.inv(cov)\n",
    "    sign, logdet = np.linalg.slogdet(cov)\n",
    "\n",
    "    # Quadratic form (one per row)\n",
    "    quad = np.einsum('ij,jk,ik->i', diff, inv, diff)  # (n,)\n",
    "\n",
    "    # Log-domain normalizer and pdf\n",
    "    const = -0.5 * d * np.log(2.0 * np.pi)\n",
    "    log_norm = const - 0.5 * logdet\n",
    "    log_pdf = log_norm - 0.5 * quad\n",
    "    pdf = np.exp(log_pdf)\n",
    "    return pdf\n",
    "\n",
    "def gmm_em(X, k, max_iter=50, tol=1e-4):\n",
    "    n, d = X.shape\n",
    "    reg = 1e-6\n",
    "\n",
    "    weights = np.ones(k) / k\n",
    "    means = X[np.random.choice(n, k, replace=False)]\n",
    "    covs = [np.cov(X.T) + np.eye(d) * reg for _ in range(k)]\n",
    "\n",
    "    log_lik_old = -np.inf\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        weighted = np.column_stack([weights[j] * gaussian_pdf(X, means[j], covs[j]) for j in range(k)])\n",
    "        row_sums = weighted.sum(axis=1, keepdims=True) + 1e-12\n",
    "        resp     = weighted / row_sums\n",
    "        log_lik  = float(np.sum(np.log(row_sums)))\n",
    "\n",
    "        Nk      = resp.sum(axis=0)\n",
    "        weights = Nk / n\n",
    "        means   = (resp.T @ X) / Nk[:, None]\n",
    "\n",
    "        for j in range(k):\n",
    "            diff    = X - means[j]\n",
    "            covs[j] = ((resp[:, j][:, None] * diff).T @ diff) / Nk[j] + reg*np.eye(d)\n",
    "\n",
    "        if abs(log_lik - log_lik_old) < tol:\n",
    "            break\n",
    "        log_lik_old = log_lik\n",
    "\n",
    "    return weights, means, covs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykMomArL9v-S"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL - (unless you want to play around with the components param)\n",
    "\n",
    "\n",
    "# Fit GMMs (use 3 components for flexibility)\n",
    "weights_skin, means_skin, covs_skin = gmm_em(skin_cbcr,3)\n",
    "weights_non, means_non, covs_non = gmm_em(non_skin_cbcr, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVaEQct3VQEw"
   },
   "source": [
    "Classify Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gBvfYxkVSQ8"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL (you may experiment with hyperparams here if you want (e.g threshold))\n",
    "\n",
    "def gmm_likelihood(X, weights, means, covs):\n",
    "    k = len(weights)\n",
    "    lik = np.zeros(X.shape[0])\n",
    "    for i in range(k):\n",
    "        lik += weights[i] * gaussian_pdf(X, means[i], covs[i])\n",
    "    return lik\n",
    "\n",
    "lik_skin = gmm_likelihood(test_cbcr, weights_skin, means_skin, covs_skin)\n",
    "lik_non = gmm_likelihood(test_cbcr, weights_non, means_non, covs_non)\n",
    "ratio = lik_skin / (lik_non + 1e-10)\n",
    "threshold = 5 # Adjust as needed\n",
    "skin_mask = (ratio > threshold).reshape(img_skin.shape[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRBY-C2CUGLr"
   },
   "source": [
    "Visualize Skin Regions and Candidate Face Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7nnU9qPUD0k"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL\n",
    "\n",
    "plt.imshow(skin_mask, cmap='gray')\n",
    "plt.title('Skin Detection Mask')\n",
    "plt.show()\n",
    "\n",
    "contours, _ = cv2.findContours(skin_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "img_with_boxes = cv2.cvtColor(img_skin, cv2.COLOR_RGB2BGR)\n",
    "for cnt in contours:\n",
    "    if cv2.contourArea(cnt) > 250:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        cv2.rectangle(img_with_boxes, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Candidate Face Bounding Boxes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dthqk1WZZ851"
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CELL - Compute per-pixel probability of skin\n",
    "\n",
    "prob_skin = lik_skin / (lik_skin + lik_non + 1e-10)\n",
    "prob_skin = prob_skin.reshape(img_skin.shape[:2])\n",
    "\n",
    "img_with_boxes = cv2.cvtColor(img_skin, cv2.COLOR_RGB2BGR)\n",
    "for cnt in contours:\n",
    "    if cv2.contourArea(cnt) > 250:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "\n",
    "        region_prob = prob_skin[y:y+h, x:x+w]\n",
    "        avg_prob = region_prob.mean()\n",
    "\n",
    "        cv2.rectangle(img_with_boxes, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        cv2.putText(\n",
    "            img_with_boxes,\n",
    "            f\"{avg_prob*100:.1f}%\",\n",
    "            (x, y - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.6,\n",
    "            (0, 255, 0),\n",
    "            2\n",
    "        )\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Candidate Face Bounding Boxes with Probabilities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrYzYAJf0lxN"
   },
   "source": [
    "---\n",
    "## Analytical Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zutJ7zsz0tBb"
   },
   "outputs": [],
   "source": [
    "# Use this space and add more cells below this to supplement the answers that you give to the questions that follow\n",
    "# This is RECOMMENDED. If you feel a question does not require additional analysis, you may refer to the coding tasks above\n",
    "# Grounding your responses in empirical evidence is standard practice in AI/ML research, so it is important that you support your results with evidence from either:\n",
    "\n",
    "#       - the coding tasks above\n",
    "#       - novel analysis conducted below\n",
    "#       - if questions are of a more foundational nature, then reference the underlying mathematics to back your response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmqGXBjs0z8N"
   },
   "outputs": [],
   "source": [
    "# Your code here (optional)\n",
    "#---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifjVw8KK0f4q"
   },
   "source": [
    "<span style=\"color: green; font-size: 20px;\">**Question 3a:**</span> Investigate how changing the number of Gaussian components (e.g., from 2 to 5) in the GMM for skin and non-skin models impacts the likelihood ratio classification accuracy, using the same threshold.\n",
    "\n",
    "Answer: going from 2 to 5 Gaussians generally makes the skin and non-skin models fit the color space more closely so the likelihood ratio separates them better in tricky tones like shadows and highlights accuracy usually rises from 2 to 3 or 4 then plateaus or even dips at 5 because components start overfitting small blobs in the background with the same fixed threshold the overfit model can boost both true positives and a few false positives so the best k is the smallest one that captures the visible modes in your image set for your masks and boxes that looked clean at k=3 and k=4 while k=5 did not clearly help\n",
    "\n",
    "<span style=\"color: green; font-size: 20px;\">**Question 3b:**</span> Explore the effects of varying the EM convergence tolerance (tol, e.g., from 1e-4 to 1e-6) and maximum iterations (e.g., 50 vs. 100) on GMM fitting time and skin mask quality, assessing if tighter tolerances reduce false positives in bounding box detection.\n",
    "\n",
    "Answer: tighter tol and larger max_iter let EM inch closer to a local optimum so fitting time goes up clearly but mask quality improves only a little if at all with tol from 1e-4 to 1e-6 you may see fewer speckles because covariances stabilize and responsibilities become crisper but the gain is small compared with the extra time a practical recipe is keep tol at 1e-4 and raise max_iter only if you see the log-likelihood still moving at the end if masks are noisy prefer better features or postprocessing over forcing EM to run longer\n",
    "\n",
    "\n",
    "<span style=\"color: green; font-size: 20px;\">**Question 3c:**</span> Detail how the Gaussian probability density function (PDF) formula is coded in gaussian_pdf using matrix operations (diff @ inv_cov * diff), and explain why regularization (np.eye(d) * 1e-6) is added to the covariance matrix.\n",
    "\n",
    "Answer: gaussian_pdf builds the multivariate normal in vectorized form X is n by d mean is 1 by d cov is d by d we compute diff = X \u2212 mean then the Mahalanobis term as quad = diff @ inv_cov @ diff^T but written with einsum so it returns an n-vector we also use slogdet to get log determinant for the normalizing constant the final pdf is exp of the log density regularization np.eye(d)*1e-6 is added to each covariance to avoid near singular matrices when color channels are correlated or a component has very few points without that the inverse or determinant can blow up and the pdf becomes unstable\n",
    "\n",
    "\n",
    "<span style=\"color: green; font-size: 20px;\">**Question 3d:**</span> Break down the Expectation-Maximization (EM) algorithm's E-step (responsibilities via likelihoods) and M-step (updating weights, means, covariances) as implemented in gmm_em, and how it mathematically models multi-modal distributions compared to single Gaussians.\n",
    "\n",
    "Answer: in the E-step we evaluate each component\u2019s likelihood weight_j * N(x | mean_j, cov_j) for every pixel then normalize across j to get responsibilities \u03b3_{ij} these are soft assignments in the M-step we update weights as Nk/n where Nk is the sum of responsibilities for component j update means as the responsibility weighted average of X and update covariances as the responsibility weighted scatter plus a tiny diagonal regularizer repeating E and M lets a mixture of Gaussians model several color modes at once unlike a single Gaussian which would smear skin colors into one ellipsoid the mixture can place separate blobs for light skin shadowed skin and warm highlights which is why it works better for real photos\n",
    "\n",
    "<span style=\"color: green; font-size: 20px;\">**Question 3e:**</span> Evaluate the likelihood ratio threshold's role in the skin mask: Why might a threshold of 1.0 result in extraneous bounding boxes (e.g., any small boxes that you might be seeing), and does adjusting it trade off sensitivity for specificity in face candidate detection?\n",
    "\n",
    "Answer: the likelihood ratio threshold decides which pixels count as skin if the threshold is 1.0 many pixels with weak evidence can still pass when the skin model has slightly higher likelihood than the non-skin model that creates extra small boxes around random patches lowering the threshold increases sensitivity and finds more faces but adds clutter raising it trims small boxes and keeps only strong skin regions the tradeoff is classic sensitivity vs specificity pick the threshold by checking precision-recall on a few images and also add simple filters like minimum area aspect ratio and dilation-erosion to suppress stray boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y_EB1c-1s8n"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GPbDR997t-n"
   },
   "source": [
    "---**END OF PA2**---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}